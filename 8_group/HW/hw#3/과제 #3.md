# 과제 #3

## 8조 (김은미, 김승현)

---

## **1) 기계 학습에서 학습이란 무엇인지를 정리하시오**(2점). 
**(가중치, 손실함수가 무엇인지를 정리하고, 데이터, 가중치, 손실함수를 이용하여 학습이 무엇인지를 정리함.)**

기계의 학습은 인간처럼 무언가를 이해하는 것이 아니라 매개변수(parameter) 또는 가중치(weigth)의 형태로 나타난다.

### **<그렇다면 가중치란 무엇인가>**

- 처음 들어오는 데이터(입력층)에서 다음 노드로 넘어갈 때, 모두 같은 값이면 계속 같은 값이 나올 것이다. 각기 다르게 곱해야 한다는 것이 바로 가중치(weight)이다.

- 예를 들어 수능 점수를 예측하려고 한다. 그렇다면 학습 데이터가 필요하다.
    
    4~9월 모의고사를 통해 수능 점수를 예측한다고 한다면 4, 5, 6월의 점수보다는 7, 8, 9월의 점수가 수능 점수와 비슷할 가능성이 높다. 그렇다면 4, 5, 6월의 점수보다는 7, 8, 9월의 점수에 더 중요하게 비중을 두어야 할 것이다. 이것을 가중치(weight)라고 이해하면 쉽다.
    

<img width="452" alt="그림1" src="https://github.com/seunghyuniisme/ML/assets/145260996/b47a7725-e204-40f7-988a-bff8c3e91435">

(참고 : [https://jh2021.tistory.com/3](https://jh2021.tistory.com/3) )

- 가중치는 모델이 학습 데이터로부터 지식을 습득하고 일반화하는 데 중요한 역할을 한다.
- 또한 각 입력 신호가 결과 출력에 미치는 중요도를 조절하는 매개변수라고도 한다.

=> 기계 학습은 입력값에 대응하는 결과값에 제대로 나오도록 하는 최적의 매개변수값을 찾는 과정이며, 그때 학습의 결과물은 매개변수(=가중치) 값이다.

### **<그렇다면 손실 함수란?>**

- 기계 학습에서는 오차를 손실(loss) 또는 비용(cost), 오차(error)라고 하며
오차를 정의한 함수를 손실 함수(loss function) 또는 비용 함수(cost function), 오차 함수(error function)라고 한다. (여기서 오차는 출력값과 사용자가 원하는 출력값의 오차를 의미한다.)
- 다시 말하면 손실 함수는 측정한 데이터를 토대로 산출한 모델의 예측값과 실제값의 차이를 표현하는 지표이다.
- 기계 학습에서는 모델 학습을 시키면 평가지표로서 손실 함수가 등장한다.
- 손실 함수는 정답(y)와 예측(^y)를 입력으로 받아 실숫값 점수를 만드는데,이때 이 점수가 높을수록 모델이 안 좋은 것이다.반대로 손실 함수의 값이 작아질수록 모델은 예측을 더 잘하게 된다.
- 그래서 기계 학습에서 성능을 향상시키기 위해선 손실 함수를 최소화 시키는 방안을 찾아야 한다.
- 손실 함수의 값이 최소화가 되도록 하는 가중치(weight)와 편향(bias)을 찾는 것이 기계 학습의 목표라고 할 수 있으며, 적절한 손실 함수를 설정해야 좋은 결과를 얻을 수 있다.

- 손실 함수에는 다양한 종류들이 있지만
    1. 회귀 모델에 쓰이는 손실함수에는MSE, MAE, RMES 등이 있고,
    2. 분류 모델에 쓰이는 손실함수에는Binary cross-entropy, Categorical cross-entropy 등이 있다.
    
- 그 중 MSE : Mean Square Error의 약자로 평균 제곱 오차를 뜻한다.
    
    
    ![그림2.png](%E1%84%80%E1%85%AA%E1%84%8C%E1%85%A6%20#3%20547ba7d882ba48dc8ee4b5de3c6af84b/%25EA%25B7%25B8%25EB%25A6%25BC2.png)
    
    ![그림3.png](%E1%84%80%E1%85%AA%E1%84%8C%E1%85%A6%20#3%20547ba7d882ba48dc8ee4b5de3c6af84b/%25EA%25B7%25B8%25EB%25A6%25BC3.png)
    
    - 현재 신경망이 훈련데이터를 얼마나 잘 처리하지 못 하느냐를 나타내며
    - 최적의 매개변수 값을 탐색하는 지표이기도 하다.
    
    - 장점 : 실제 정답에 대한 정답률의 오차뿐만 아니라 다른 오답에 대한 정답률의 오차도 포함하여 계산해준다.
    - 단점 : 값을 제곱하기 때문에 절댓값이 1미만인 값은 더 작아지고, 1보다 큰 값은 더 커지는 왜곡이 발생할 수 있다. 즉, 제곱하기 때문에 특이값의 영향을 많이 받는다.

### **<기계 학습에서 학습이란?>**

- 학습한다는 것은 자료(데이터)를 통해서 새로운 것을 알아간다는 의미이다.
기계 학습, 즉 기계가 ‘학습’을 한다는 것은 입력값(데이터)에 대응하는 결과값이 제대로 나오도록 하는 
최적의 매개변수(=가중치)를 찾는 과정이다.
- 기계 학습에서 학습이란 모델이 데이터를 통해 가중치를 조정하고 손실 함수를 최소화하도록 하는 과정이다.

## 2) 확률적 경사 하강법의 소스 코드를 분석하시오(2점). 
(Page 173, 4장 모델 훈련, 첨부 파일 참조)

![Untitled](%E1%84%80%E1%85%AA%E1%84%8C%E1%85%A6%20#3%20547ba7d882ba48dc8ee4b5de3c6af84b/Untitled.jpeg)

```python
def learning_schedule(t):    #학습 스케줄을 계산하는 함수로 에포크와 미니배치 스텝을 이용하여 현재 학습률을 계산
	return t0 / (t + t1)       #학습률이 감소하도록 함

theta = np.random.randn(2,1)  #무작위 초기화
               #2x1 크기의 theta 벡터로 표현되며, 선형 회귀 모델의 가중치를 나타냄 

for epoch in range(n_epochs):  #에포크 루프 시작
     for i in range(m):    #데이터셋 크기 만큼 반복하며 미니배치 스텝 수행
         random_index = np.random.randint(m)  #데이터셋에서 임의의 인덱스를 선택하여 미니배치를 생성
         xi = X_b[random_index:random_index+1]  #xi와 yi는 선택된 미니배치의 특성과 레이블
         yi = Y[random_index:random_index+1]
       gradients = 2 * xi.T.dot(xi.dot(theta) - yi)   #현재 미니배치에 대한 그래디언트 벡터를 계산
       eta = learning_schedule(epoch * m + i)   #학습 스케줄을 사용하여 현재 스텝에서의 학습률을 계산
       theta = theta = eta *gradients   #theta를 업데이트하여 모델의 파라미터를 조정

* 주어진 에포크 만큼 반복하고, 각 에포크에서 데이터셋을 무작위로 섞어 미니 배치를 생성하며
확률적 경사 하강법을 사용하여 모델을 학습한다. 학습률은 에포크와 스텝에 따라 조절되며 
이를 통해 모델이 미니 배치에 대한 그래디언트를 사용하여 점진적으로 학습하고 전체 데이터셋을 반복하여 
손실을 최소화하는 방향으로 모델 파라미터를 조절한다.
```
