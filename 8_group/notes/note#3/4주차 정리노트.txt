(8조 김은미, 김승현)


1. SGD 알고리즘 코드 구현 

* 코드 작성 : 김승현
* 코드 작성 도움 및 정리 노트 작성 

은미 : 교수님이 Epoch 얘기 하셨으니까 이번 코드에도 그걸 이용해서 작성을 해야될 것 같아요
n_epochs = 50 이라는 건 횟수가 50번이라는 뜻이고요
승현 : 음 t0, t1이 학습 스케줄 하이퍼파라미터라고 하는데 학습 스케줄 하이퍼파라미터라는 게 
알고리즘 사용자가 경험에 의해서 직접 세팅하는 값이고

* 샘플을 무작위로 선택하기 때문에 어떤 한 샘플은 여러 번 선택 될 수도 있고 
어떤 샘플은 선택되지 못할 수도 있다
알고리즘의 모든 샘플을 사용하게 하려면 training sets를 섞은 후 차례대로 하나씩 선택하고
다음 epoch에서 다시 섞는 방식 적용 가능.

* 코드는 아까 했던 배치랑 비슷함


2. <문제> 데이터셋 1000개, 에폭 100번, 미니 배치는 50개일 때,
가중치 업데이트는 몇 번인지.

1 에폭(epoch)의 가중치 = 데이터 개수 / 미니배치

1000 / 50 = 20이고, 이때 20은 한 에폭 당 가중치가 된다
에폭이 100번이므로 20 * 100 = 2000

=> 가중치 업데이트는 2000번



+++ 어려워서 따로 찾아본 것 +++
1. 미니배치 경사 하강법 : 행렬 연산에 최적화된 하드웨어, 특히 GPU를 사용해서 얻는 성능 향상
미니 배치를 증가시키면 파라미터 공간에서 SGD보다 덜 불규칙하게 움직인다..??
(참고: https://better-tomorrow.tistory.com/entry/Mini-batch-Gradient-Descent미니배치-경사-하강법)


2. 미니배치는 SGD(확률적 경사 하강법)와 배치를 섞은 것.
전체 데이터를 N등분하여 각각의 학습 데이터를 배치 방식으로 학습시킨다. 
따라서 최대한 신경망을 한 번 학습시키는데 걸리는 시간을 줄이면서 전체 데이터를 반영할 수 있게 되어
효율적으로 CPU, GPU를 활용할 수 있게 된다.
(참고 : https://welcome-to-dewy-world.tistory.com/86)



