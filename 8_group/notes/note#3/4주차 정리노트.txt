(8조 김은미, 김승현)


1. SGD 알고리즘 코드 구현 

* 코드 작성 : 김승현
* 코드 작성 도움 및 정리 노트 작성 

은미 : 교수님이 Epoch 얘기 하셨으니까 이번 코드에도 그걸 이용해서 작성을 해야될 것 같아요
n_epochs = 50 이라는 건 횟수가 50번이라는 뜻이고요
승현 : 음 t0, t1이 학습 스케줄 하이퍼파라미터라고 하는데 학습 스케줄 하이퍼파라미터라는 게 
알고리즘 사용자가 경험에 의해서 직접 세팅하는 값이고

* 샘플을 무작위로 선택하기 때문에 어떤 한 샘플은 여러 번 선택 될 수도 있고 
어떤 샘플은 선택되지 못할 수도 있다
알고리즘의 모든 샘플을 사용하게 하려면 training sets를 섞은 후 차례대로 하나씩 선택하고
다음 epoch에서 다시 섞는 방식 적용 가능.

* 코드는 아까 했던 배치랑 비슷함


2. <문제> 데이터셋 1000개, 에폭 100번, 미니 배치는 50개일 때,
가중치 업데이트는 몇 번이 변경될까.



