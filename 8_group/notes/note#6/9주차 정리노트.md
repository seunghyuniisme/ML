# 9주차 정리노트

## 8조(김은미, 김승현)

---

### 7장. 앙상블 학습과 랜덤 포레스트

### 1. 앙상블 학습과 랜던 포레스트

- 앙상블 : 여러 개의 예측기로 이뤄진 그룹
- **앙상블 학습 : 여러 개의 분류기를 생성하고 각 예측들을 결합함으로써 보다 정확한 예측을 도출하는 기법**

### 2. 투표 기반 분류기

- 투표 기반 분류기 : 동일 훈련 세트에 대해 여러 종류의 분류기 이용한 앙상블 학습 적용 후
직접 또는 간접 투표를 통해 예측값 결정
- 투표식 분류기의 특징
    - 앙상블에 포함된 분류기들 사이의 독립성이 전재되는 경우 개별 분류기 보다 정확한 예측 가능
- 직접투표: 앙상블에 포함된 예측기들의 예측값을 다수결 투표로 결정
간접투표: 앙상블에 포함된 예측기들의 예측한 확률값들의 평균값으로 예측값 결정
전제: 모든 예측기가 predict_proba() 메서드와 같은 확률 예측 기능을 지원해야 함

- **사이킷런의 투표기반 분류기(Voting Classifier) 코드(7.1.2)**
    
    ```python
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.ensemble import VotingClassifier
    from sklearn.linear_model import LogisticRegression
    from sklearn.svm import SVC
    
    log_clf = LogisticRegression(solver="lbfgs", random_state=42)
    rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)
    svm_clf = SVC(gamma="scale", random_state=42)
    
    voting_clf = VotingClassifier(
        estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],
        voting='hard')
    
    * 이때 voting='hard'이면 직접 투표 방식 지정 하이퍼 파라미터
    ('soft'로 설정하면 간접 투표 방식이 된다.)
    ```
    

### 3. 배깅과 페이스팅

- 동일한 예측기를 훈련 세트의 다양한 부분집합을 대상으로 학습시키는 방식
- 훈련세트에서 중복을 허용하여 샘플링하는 방식을 **배깅(bagging),**
중복을 허용하지 않고 샘플링하는 방식을 **페이스팅(pasting)** 이라고 한다

- 사이킷런의 배깅과 페이스팅 실습(7.2.1)
    
    ```python
    from sklearn.ensemble import BaggingClassifier
    from sklearn.tree import DecisionTreeClassifier
    
    bag_clf = BaggingClassifier(
        DecisionTreeClassifier(), n_estimators=500,
        max_samples=100, bootstrap=True, random_state=42)
    bag_clf.fit(X_train, y_train)
    y_pred = bag_clf.predict(X_test
    
    * 이때 bootstrap=True이면 배깅, bootstrap=False이면 페이스팅.
    ```
    
    ```python
    tree_clf = DecisionTreeClassifier(random_state=42)
    tree_clf.fit(X_train, y_train)
    y_pred_tree = tree_clf.predict(X_test)
    print(accuracy_score(y_test, y_pred_tree))
    ```
    

![Untitled](9%E1%84%8C%E1%85%AE%E1%84%8E%E1%85%A1%20%E1%84%8C%E1%85%A5%E1%86%BC%E1%84%85%E1%85%B5%E1%84%82%E1%85%A9%E1%84%90%E1%85%B3%208dece0628c634a91b1038b31950234ba/Untitled.png)

- 배깅 vs 페이스팅
    - 배깅 방식이 편향을 키우고 분산은 줄임.
    - 페이스팅보다는 배깅 방식이 좀 더 나은 모델 생성

- 앙상블 학습의 편향과 분산
    - 개별 예측기의 경우에 비해 편향은 비슷하지만 분산은 줄어듦 (과대적합의 위험성 down)
    - 개별 예측기: 배깅/페이스팅 방식으로 학습하면 전체 훈련 세트를 대상으로 학습한 경우에
    비해 편향이 커짐 (과소적합 위험성 up)

### 4. oob 평가

- oob = Out-of-Bag : 선택되지 않은 훈련 샘플
- oob 샘플 활용해 앙상블 학습에 사용된 개별 예측기의 성능 평가 가능

### 5. 랜덤 포레스트

- 배깅/페이스팅 방법을 적용한 <결정트리의 앙상블>을 최적화한 모델
- 배깅/페이스팅 방법을 적용한 결정트리의 앙상블을 최적화한 모델
- 랜덤 포레스트는 결정 배깅과 비슷함

- 랜덤 포레스트 알고리즘
    
    ```python
    from sklearn.ensemble import RandomForestClassifier
    
    rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, random_state=42)
    rnd_clf.fit(X_train, y_train)
    
    y_pred_rf = rnd_clf.predict(X_test)
    ```
    

- BaggingClassifier를 사용해 RandomForestClassifier와 유사하게 만든 소스
    
    ```python
    bag_clf = BaggingClassifier(
        DecisionTreeClassifier(max_features="sqrt", max_leaf_nodes=16),
        n_estimators=500, random_state=42)
    ```
    
    ```python
    bag_clf.fit(X_train, y_train)
    y_pred = bag_clf.predict(X_test)
    ```
    

### 6. 특성 중요도

- 해당 특성을 사용한 노드가 평균적으로 불순도를 얼마나 감소시키는 지를 측정
- 불순도를 많이 줄이면 그만큼 중요도가 커짐
- 코드
    
    ```python
    from sklearn.datasets import load_iris
    iris = load_iris()
    rnd_clf = RandomForestClassifier(n_estimators=500, random_state=42)
    rnd_clf.fit(iris["data"], iris["target"])
    for name, score in zip(iris["feature_names"], rnd_clf.feature_importances_):
        print(name, score)
    ```
    
    ```python
    rnd_clf.feature_importances_
    ```
    
    ?
    

### 7. 에이다부스트

- 좀 더 나은 예측기를 생성하기 위해 잘못 적용된 가중치를 조정하여 새로운 예측
기를 추가하는 앙상블 기법

### 8. 그레이디언트 부스팅

- 에이다 부스트와 동일
샘플의 가중치를 수정하는 대신 이전 예측기가 만든 잔여 오차에 대해 새로운 에측기를 학습시킴
(잔여 오차: 예측값과 실제값 사이의 오차)
- 잔여 오차를 줄이는 방향으로 모델을 학습시키는데, 경사하강법을 사용하여 최적화

### 9. 스태킹

- 앙상블에 속한 모든 예측기의 예측을 취합하는 간단한 함수(직접 투표 같은)를 사용하는 대신 취합하는 모델을 훈련시킬 수 없을까?
    - 사이킷런 직접 지원x
    - 블렌더 또는 메타 학습기
    - 블렌더를 학습시키는 일반적인 방법은 홀드 아웃 세트를 사용

- 편향과 분산
예측값들과 정답이 대체로 멀리 떨어져 있으면 결과의 편향(bias)이 높다고 말하고,
예측값들이 자기들끼리 대체로 멀리 흩어져있으면 결과의 분산(variance)이 높다고 말한다.